name: Update Notion Cache

on:
  schedule:
    - cron: '0 0 * * *'  # Runs at 00:00 UTC daily
  workflow_dispatch:      # Allows manual trigger

jobs:
  update-cache:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests python-dotenv tenacity

    - name: Create update_cache.py
      run: |
        cat > update_cache.py << 'EOL'
        import os
        import json
        import time
        import requests
        from pathlib import Path
        from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

        # Direct values since they're public
        NOTION_TOKEN = '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'
        SUBJECT_DATABASE_ID = '2674b67cbdf84a11a057a29cc24c524f'
        PHARMACOLOGY_DATABASE_ID = '9ff96451736d43909d49e3b9d60971f8'
        ETG_DATABASE_ID = '22282971487f4f559dce199476709b03'
        ROTATION_DATABASE_ID = '69b3e7fdce1548438b26849466d7c18e'
        TEXTBOOKS_DATABASE_ID = '13d5964e68a480bfb07cf7e2f1786075'
        GUIDELINES_DATABASE_ID = '13d5964e68a48056b40de8148dd91a06'

        class NotionCache:
            def __init__(self):
                self.cache_dir = Path("cache")
                self.cache_dir.mkdir(exist_ok=True)
                self.headers = {
                    "Authorization": f"Bearer {NOTION_TOKEN}",
                    "Notion-Version": "2022-06-28",
                    "Content-Type": "application/json"
                }

            @retry(
                stop=stop_after_attempt(5),
                wait=wait_exponential(multiplier=1, min=4, max=60),
                retry=retry_if_exception_type((requests.exceptions.RequestException, requests.exceptions.Timeout))
            )
            def fetch_pages_batch(self, database_id: str, start_cursor: str = None) -> dict:
                """Fetch a single batch of pages with retry logic"""
                payload = {
                    "filter": {
                        "property": "For Search",
                        "checkbox": {
                            "equals": True
                        }
                    },
                    "page_size": 100
                }
                if start_cursor:
                    payload["start_cursor"] = start_cursor

                response = requests.post(
                    f"https://api.notion.com/v1/databases/{database_id}/query",
                    headers=self.headers,
                    json=payload,
                    timeout=30  # Set explicit timeout
                )
                response.raise_for_status()
                return response.json()

            def update_cache(self, database_id: str):
                """Fetch and save all pages from a Notion database to the cache"""
                pages = []
                has_more = True
                start_cursor = None
                fetch_success = True

                while has_more:
                    try:
                        data = self.fetch_pages_batch(database_id, start_cursor)
                        pages.extend(data['results'])
                        has_more = data.get('has_more', False)
                        start_cursor = data.get('next_cursor')
                        print(f"Successfully fetched batch of {len(data['results'])} pages")
                    except Exception as e:
                        print(f"Error fetching from Notion after all retries: {e}")
                        fetch_success = False
                        break

                if not fetch_success:
                    # Raise exception to trigger GitHub Actions retry
                    raise Exception(f"Failed to fetch complete data for database {database_id}")

                cache_path = self.cache_dir / f"{database_id}.json"
                try:
                    with cache_path.open('w', encoding='utf-8') as f:
                        json.dump({
                            'version': 1,
                            'timestamp': time.time(),
                            'pages': pages
                        }, f)
                    print(f"Saved {len(pages)} pages to cache for database {database_id}")
                except Exception as e:
                    print(f"Error saving cache: {e}")
                    raise  # Raise exception to trigger GitHub Actions retry

        def update_notion_cache():
            """Update all Notion database caches"""
            notion_cache = NotionCache()
            databases = [
                SUBJECT_DATABASE_ID,
                PHARMACOLOGY_DATABASE_ID,
                ETG_DATABASE_ID,
                ROTATION_DATABASE_ID,
                TEXTBOOKS_DATABASE_ID,
                GUIDELINES_DATABASE_ID
            ]

            for database_id in databases:
                notion_cache.update_cache(database_id)
                print(f"Successfully updated cache for database {database_id}")
                # Add small delay between databases to avoid rate limiting
                time.sleep(2)

        if __name__ == "__main__":
            update_notion_cache()
        EOL

    - name: Update cache
      uses: nick-fields/retry@v2
      with:
        timeout_minutes: 30
        max_attempts: 3
        command: python update_cache.py
        on_retry_command: |
          echo "Retry attempt in progress..."
          sleep 10  # Add delay before retry

    - name: Commit and push if changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add cache/
        git commit -m "Update cache" -a || exit 0
        git push
